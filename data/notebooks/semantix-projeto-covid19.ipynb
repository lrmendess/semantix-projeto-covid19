{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listagem dos arquivos CSV que foram importados para o HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/hist_painel_covidbr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação da tabela particionada por município"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = (\n",
    "    spark.read\n",
    "    .option('header', True)\n",
    "    .option('inferSchema', True)\n",
    "    .option('encoding', 'UTF-8')\n",
    "    .option('delimiter', ';')\n",
    "    .format('csv')\n",
    "    .load('/user/hive/warehouse/hist_painel_covidbr')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_csv.write\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy('municipio')\n",
    " .saveAsTable('painel_covidbr.hist_por_municipio'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/painel_covidbr.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura da tabela particionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read\n",
    "    .table('painel_covidbr.hist_por_municipio')\n",
    "    .select(\n",
    "        col('regiao'),\n",
    "        col('estado'),\n",
    "        col('municipio'),\n",
    "        col('populacaoTCU2019').alias('populacao'),\n",
    "        col('casosAcumulado'),\n",
    "        col('casosNovos'),\n",
    "        col('obitosAcumulado'),\n",
    "        col('obitosNovos'),\n",
    "        col('Recuperadosnovos').alias('recuperadosNovos'),\n",
    "        col('emAcompanhamentoNovos'),\n",
    "        col('data')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação da tabela intermediária de síntese de casos por Estados\n",
    "\n",
    "Tornou-se necessária por limitações de hardware do equipamento do autor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, desc, lit, row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_regioes_estados = Window.partitionBy('regiao', 'estado').orderBy(desc('data'))\n",
    "\n",
    "df_estados = (\n",
    "    df\n",
    "    .withColumn('row', row_number().over(w_regioes_estados))\n",
    "    .filter(col('row') == 1)\n",
    "    .drop('row')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sintese_casos_estados = (\n",
    "    df_estados\n",
    "    .withColumn('incidencia', (col('casosAcumulado') / col('populacao')) * lit(100_000))\n",
    "    .withColumn('mortalidade', (col('obitosAcumulado') / col('populacao')) * lit(100_000))\n",
    "    .withColumn('letalidade', col('obitosAcumulado') / col('casosAcumulado'))\n",
    "    .select(\n",
    "        col('regiao'),\n",
    "        col('estado'),\n",
    "        col('recuperadosNovos'),\n",
    "        col('emAcompanhamentoNovos'),\n",
    "        col('casosAcumulado'),\n",
    "        col('casosNovos'),\n",
    "        col('incidencia'),\n",
    "        col('obitosAcumulado'),\n",
    "        col('obitosNovos'),\n",
    "        col('letalidade'),\n",
    "        col('mortalidade'),\n",
    "        col('populacao'),\n",
    "        col('data')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_sintese_casos_estados.write\n",
    " .mode('overwrite')\n",
    " .partitionBy('estado')\n",
    " .saveAsTable('painel_covidbr.sintese_casos_estados'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Painel da COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, col, asc, desc, lit, sum, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regioes_estados = spark.read.table('painel_covidbr.sintese_casos_estados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização do painel da COVID-19 no Brasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brasil = df_regioes_estados.filter(col('regiao') == 'Brasil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Visão 1) - Casos recuperados e em acompanhamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_visao1 = (\n",
    "    df_brasil\n",
    "    .select(\n",
    "        col('recuperadosNovos'),\n",
    "        col('emAcompanhamentoNovos')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visao1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_visao1.write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('painel_covidbr.painel_casos_recuperados_acompanhamento'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table('painel_covidbr.painel_casos_recuperados_acompanhamento').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Visão 2) - Casos confirmados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visao2 = (\n",
    "    df_brasil\n",
    "    .select(\n",
    "        col('casosAcumulado'), \n",
    "        col('casosNovos'), \n",
    "        col('incidencia')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visao2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_visao2.write\n",
    " .mode('overwrite')\n",
    " .option('compression', 'snappy')\n",
    " .parquet('/user/hive/warehouse/casos_confirmados_parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/casos_confirmados_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Visão 3) - Óbitos confirmados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "df_visao3 = (\n",
    "    df_brasil\n",
    "    .select(to_json(struct(\n",
    "        col('obitosAcumulado'),\n",
    "        col('obitosNovos'),\n",
    "        col('letalidade'),\n",
    "        col('mortalidade'))).alias(\"value\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visao3.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_visao3.write\n",
    " .format('kafka')\n",
    " .option('kafka.bootstrap.servers', 'kafka:29092')\n",
    " .option('topic', 'covid19-obitos')\n",
    " .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar o resultado do kafka, execute:\n",
    "\n",
    "```bash\n",
    "sh scripts/teste-kafkaconsumer.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização da síntese de casos de COVID-19 no Brasil e Regiões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Visão 1) - Síntese de casos do Brasil e Regiões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sintese_regioes = (\n",
    "    df_regioes_estados\n",
    "    .groupBy('regiao')\n",
    "    .agg(\n",
    "        sum('casosAcumulado').alias('casosAcumulado'),\n",
    "        sum('obitosAcumulado').alias('obitosAcumulado'),\n",
    "        sum('populacao').alias('populacao'),\n",
    "        max('data').alias('data')\n",
    "    )\n",
    "    .withColumn('incidencia', (col('casosAcumulado') / col('populacao')) * lit(100_000))\n",
    "    .withColumn('mortalidade', (col('obitosAcumulado') / col('populacao')) * lit(100_000))\n",
    "    .select(\n",
    "        col('regiao'),\n",
    "        col('casosAcumulado'),\n",
    "        col('obitosAcumulado'),\n",
    "        col('incidencia'),\n",
    "        col('mortalidade'),\n",
    "        col('data')\n",
    "    )\n",
    "    .orderBy(desc('casosAcumulado'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sintese_regioes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+---------------+-----------------+------------------+-------------------+\n",
      "|      regiao|casosAcumulado|obitosAcumulado|       incidencia|       mortalidade|               data|\n",
      "+------------+--------------+---------------+-----------------+------------------+-------------------+\n",
      "|      Brasil|      18855015|         526892| 8972.29262594004|250.72529543290204|2021-07-06 00:00:00|\n",
      "|     Sudeste|       7138803|         245311| 8078.17951758234| 277.5908363961915|2021-07-06 00:00:00|\n",
      "|    Nordeste|       4455737|         107824| 7807.26803537182|188.92741394878797|2021-07-06 00:00:00|\n",
      "|         Sul|       3611041|          80705|12046.44691563753| 269.2321960139824|2021-07-06 00:00:00|\n",
      "|Centro-Oeste|       1916619|          49207|11760.50989275744| 301.9376361670813|2021-07-06 00:00:00|\n",
      "|       Norte|       1732815|          43845| 9401.64332010560|237.88751330640042|2021-07-06 00:00:00|\n",
      "+------------+--------------+---------------+-----------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sintese_regioes.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
