{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listagem dos arquivos CSV que foram importados para o HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/hist_painel_covidbr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação da tabela particionada por município"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add('regiao', 'string')\n",
    "    .add('estado', 'string')\n",
    "    .add('municipio', 'string')\n",
    "    .add('coduf', 'integer')\n",
    "    .add('codmun', 'integer')\n",
    "    .add('codRegiaoSaude', 'integer')\n",
    "    .add('nomeRegiaoSaude', 'string')\n",
    "    .add('data', 'timestamp')\n",
    "    .add('semanaEpi', 'integer')\n",
    "    .add('populacaoTCU2019', 'integer')\n",
    "    .add('casosAcumulado', 'integer')\n",
    "    .add('casosNovos', 'integer')\n",
    "    .add('obitosAcumulado', 'integer')\n",
    "    .add('obitosNovos', 'integer')\n",
    "    .add('Recuperadosnovos', 'integer')\n",
    "    .add('emAcompanhamentoNovos', 'integer')\n",
    "    .add('interior/metropolitana', 'integer')\n",
    ")\n",
    "\n",
    "df_csv = (\n",
    "    spark.read\n",
    "    .schema(schema)\n",
    "    .option('header', True)\n",
    "    .option('encoding', 'UTF-8')\n",
    "    .option('delimiter', ';')\n",
    "    .format('csv')\n",
    "    .load('/user/hive/warehouse/hist_painel_covidbr')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_csv.write\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy('municipio')\n",
    " .saveAsTable('painel_covidbr.hist_por_municipio'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/painel_covidbr.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura da tabela particionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read\n",
    "    .table('painel_covidbr.hist_por_municipio')\n",
    "    .select(\n",
    "        col('regiao'),\n",
    "        col('estado'),\n",
    "        col('municipio'),\n",
    "        col('populacaoTCU2019').alias('populacao'),\n",
    "        col('casosAcumulado'),\n",
    "        col('casosNovos'),\n",
    "        col('obitosAcumulado'),\n",
    "        col('obitosNovos'),\n",
    "        col('Recuperadosnovos').alias('recuperadosNovos'),\n",
    "        col('emAcompanhamentoNovos'),\n",
    "        col('data')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação da tabela intermediária de síntese de casos por Estados\n",
    "\n",
    "Tornou-se necessária por limitações de hardware do equipamento do autor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, desc, lit, row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_regioes_estados = Window.partitionBy('regiao', 'estado').orderBy(desc('data'))\n",
    "\n",
    "df_estados = (\n",
    "    df\n",
    "    .withColumn('row', row_number().over(w_regioes_estados))\n",
    "    .filter(col('row') == 1)\n",
    "    .drop('row')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sintese_casos_estados = (\n",
    "    df_estados\n",
    "    .withColumn('incidencia', (col('casosAcumulado') / col('populacao')) * lit(100_000))\n",
    "    .withColumn('mortalidade', (col('obitosAcumulado') / col('populacao')) * lit(100_000))\n",
    "    .withColumn('letalidade', col('obitosAcumulado') / col('casosAcumulado'))\n",
    "    .select(\n",
    "        col('regiao'),\n",
    "        col('estado'),\n",
    "        col('recuperadosNovos'),\n",
    "        col('emAcompanhamentoNovos'),\n",
    "        col('casosAcumulado'),\n",
    "        col('casosNovos'),\n",
    "        col('incidencia'),\n",
    "        col('obitosAcumulado'),\n",
    "        col('obitosNovos'),\n",
    "        col('letalidade'),\n",
    "        col('mortalidade'),\n",
    "        col('populacao'),\n",
    "        col('data')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_sintese_casos_estados.write\n",
    " .mode('overwrite')\n",
    " .partitionBy('estado')\n",
    " .saveAsTable('painel_covidbr.sintese_casos_estados'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Painel da COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, col, asc, desc, lit, sum, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regioes_estados = spark.read.table('painel_covidbr.sintese_casos_estados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização do painel da COVID-19 no Brasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brasil = df_regioes_estados.filter(col('regiao') == 'Brasil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Visão 1) - Casos recuperados e em acompanhamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_visao1 = (\n",
    "    df_brasil\n",
    "    .select(\n",
    "        col('recuperadosNovos'),\n",
    "        col('emAcompanhamentoNovos')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visao1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_visao1.write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('painel_covidbr.painel_casos_recuperados_acompanhamento'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table('painel_covidbr.painel_casos_recuperados_acompanhamento').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Visão 2) - Casos confirmados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visao2 = (\n",
    "    df_brasil\n",
    "    .select(\n",
    "        col('casosAcumulado'), \n",
    "        col('casosNovos'), \n",
    "        col('incidencia')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visao2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_visao2.write\n",
    " .mode('overwrite')\n",
    " .option('compression', 'snappy')\n",
    " .parquet('/user/hive/warehouse/casos_confirmados_parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/casos_confirmados_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Visão 3) - Óbitos confirmados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "df_visao3 = (\n",
    "    df_brasil\n",
    "    .select(to_json(struct(\n",
    "        col('obitosAcumulado'),\n",
    "        col('obitosNovos'),\n",
    "        col('letalidade'),\n",
    "        col('mortalidade'))).alias(\"value\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visao3.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_visao3.write\n",
    " .format('kafka')\n",
    " .option('kafka.bootstrap.servers', 'kafka:29092')\n",
    " .option('topic', 'covid19-obitos')\n",
    " .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar o resultado do kafka, execute:\n",
    "\n",
    "```bash\n",
    "sh scripts/teste-kafkaconsumer.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização da síntese de casos de COVID-19 no Brasil e Regiões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Visão 1) - Síntese de casos do Brasil e Regiões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sintese_regioes = (\n",
    "    df_regioes_estados\n",
    "    .groupBy('regiao')\n",
    "    .agg(\n",
    "        sum('casosAcumulado').alias('casosAcumulado'),\n",
    "        sum('obitosAcumulado').alias('obitosAcumulado'),\n",
    "        sum('populacao').alias('populacao'),\n",
    "        max('data').alias('data')\n",
    "    )\n",
    "    .withColumn('incidencia', (col('casosAcumulado') / col('populacao')) * lit(100_000))\n",
    "    .withColumn('mortalidade', (col('obitosAcumulado') / col('populacao')) * lit(100_000))\n",
    "    .select(\n",
    "        col('regiao'),\n",
    "        col('casosAcumulado'),\n",
    "        col('obitosAcumulado'),\n",
    "        col('incidencia'),\n",
    "        col('mortalidade'),\n",
    "        col('data')\n",
    "    )\n",
    "    .orderBy(desc('casosAcumulado'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sintese_regioes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sintese_regioes.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
