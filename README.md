# Projeto final da trilha de Engenharia de Dados da Semantix

## üõ†Ô∏è Pr√©-Requisitos

Garanta que a vari√°vel `vm.max_map_count` em `/etc/sysctl.conf` esteja igual a `262144` para o bom funcionamento do Elastic.
```bash
sudo sysctl -w vm.max_map_count=262144
```

## üöÄ Executar o ambiente
Na raiz do projeto, inicie os servi√ßos com o docker compose:
```bash
docker-compose up -d
```

**Na primeira vez que instanciar os servi√ßos**, execute o script de prepara√ß√£o do ambiente:
```bash
sh prepare-environment.sh
```

Para parar os servi√ßos, utilize:
```bash
docker-compose stop
```

Para iniciar os servi√ßos, utilize:
```bash
docker-compose start
```

Para acessar o jupyter notebook, utilize a url: [http://localhost:8889](http://localhost:8889)

## ü¶Ñ Desenvolvimento
>Obs: Era de interesse desenvolver o projeto utilizando structured streaming e dar mais aten√ß√£o para a est√©tica dos resultados, assim como melhor automatizar os processos. Por contratempos da vida, n√£o consegui tempo para terminar o projeto como gostaria at√© 08/08/2022 (prazo final). Por√©m como foi de grande aproveitamento, resolvi publicar mesmo assim.

Todo o projeto foi desenvolvido utilizando o jupyter, atrav√©s do notebook [semantix-projeto-covid19](data/notebooks/semantix-projeto-covid19.ipynb), a qual possui o passo a passo do que foi feito.

Foi utilizado o t√≥pico `covid19-obitos` do Kafka como entrada para o Logstash ([logstash.conf](pipeline/logstash.conf)), tendo como resultado de **√ìbitos Confirmados** o seguinte Dashboard:

![Dashboard no Kibana](images/dashboard-kibana.jpg)

O arquivo de importa√ß√£o do dashboard acima se encontra em [input/dashboard_covidbr/export.ndjson](input/dashboard_covidbr/export.ndjson).

### Scripts de apoio
Foram criados alguns scripts para apoiar a prepara√ß√£o do ambiente e realizar testes, s√£o eles:

|Script|Descri√ß√£o|
|---|---|
|scripts/create-hive-database.sh|Cria o banco de dados `painel_covidbr` no hive-server|
|scripts/create-kafka-topic.sh|Cria o t√≥pico `covid19-obitos` no kafka|
|scripts/fix-spark-dependencies.sh|Adiciona a depend√™ncia `parquet-hadoop-bundle-1.6.0.jar` jupyter|
|scripts/input-to-hdfs|Copia os arquivos de `input/hist_painel_covidbr` para o HDFS|
|prepare-environment.sh|Executa todos os cripts acima, em s√©rie, na primeira vez que os containers s√£o instanciados|
|scripts/test-kafka-consumer.sh|Cria um console consumer para exibir as informa√ß√µes do t√≥pico `covid19-obitos`|

Este reposit√≥rio √© um fork de [rodrigo-reboucas](https://github.com/rodrigo-reboucas/docker-bigdata)
